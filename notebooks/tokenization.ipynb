{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P']\n",
      "{' ': 0, 'L': 1, 'N': 2, 'P': 3, 'T': 4, 'a': 5, 'c': 6, 'e': 7, 'f': 8, 'g': 9, 'i': 10, 'k': 11, 'n': 12, 'o': 13, 'r': 14, 's': 15, 't': 16, 'x': 17, 'z': 18}\n",
      "[4, 13, 11, 7, 12, 10, 18, 10, 12, 9, 0, 16, 7, 17, 16, 0, 10, 15, 0, 5, 0, 6, 13, 14, 7, 0, 16, 5, 15, 11, 0, 13, 8, 0, 2, 1, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Tokenizing text is a core task of NLP\"\n",
    "\n",
    "# 文字トークン化\n",
    "tokenized = list(text)\n",
    "print(tokenized)\n",
    "\n",
    "# 数値化: 一意な整数でエンコード\n",
    "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized)))}\n",
    "print(token2idx)\n",
    "\n",
    "input_ids = [token2idx[token] for token in tokenized]\n",
    "print(input_ids)\n",
    "\n",
    "# 一意のIDの対応付けでは順序関係を付与してしまう\n",
    "# => one-hot vector へ変換\n",
    "# = トークンエンコーディング\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "input_ids = torch.tensor(input_ids)\n",
    "one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\n",
    "one_hot_encodings.shape # token_num:38, vocab_size:20\n",
    "# TensorFlow: tf.one_hot() # depthがnum_classesに相当\n",
    "one_hot_encodings[0] # 'T' => ID: 4に対応"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## サブワードトークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '[SEP]']\n",
      "[CLS] tokenizing text is a core task of nlp [SEP]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "# 手動ロード ditilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)\n",
    " \n",
    "encoded = tokenizer(text)\n",
    "print(encoded)\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded.input_ids)\n",
    "print(tokens)\n",
    "print(tokenizer.convert_tokens_to_string(tokens))\n",
    "# tokenizer.vocab_size, tokenizer.model_max_length, tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "トークン埋め込みまでされているのか微妙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
